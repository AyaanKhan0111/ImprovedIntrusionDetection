# Network Intrusion Detection with Ensemble Models

## Overview

This repository contains **two distinct implementations** of network intrusion detection systems using ensemble machine learning models:

1. **Base Implementation (base.ipynb)** - Based on Adewole et al. (2025) published in Sensors Journal
   - Datasets: CIC-IDS2017 and CICIoT2023
   - Focus: Binary and multi-class classification
   - Models: Random Forest, AdaBoost, XGBoost, LightGBM, CatBoost
   
2. **Improved Implementation (improved.ipynb)** - Enhanced version with data leakage fixes
   - Datasets: UNSW-NB15 and ToN-IoT
   - Focus: Binary classification with advanced techniques
   - Models: LightGBM, XGBoost, TabNet, River online learning

Both implementations focus on detecting network intrusions, but they use different datasets, methodologies, and evaluation approaches.

## üìä Datasets

### Base Paper Datasets (base.ipynb)

#### CIC-IDS2017 Dataset
The CIC-IDS2017 dataset is a comprehensive network intrusion detection dataset created by the Canadian Institute for Cybersecurity.

| Property | Details |
|----------|---------|
| **Source** | University of New Brunswick (UNB) |
| **Total Records** | 2,830,743 records |
| **Features** | 78 features (after removing duplicate column) |
| **Attack Categories** | 14 types (DDoS, DoS Hulk, PortScan, Bot, FTP-Patator, SSH-Patator, Web Attack, Infiltration, etc.) |
| **Normal Traffic** | BENIGN class |
| **Binary Classification** | BENIGN vs. Attack (all attack types combined) |
| **Multi-class** | 15 classes (1 BENIGN + 14 attack types) |
| **Key Features** | Flow duration, packet counts, byte counts, inter-arrival times, flags, header lengths |
| **Collection Period** | July 3-7, 2017 |

#### CICIoT2023 Dataset
The CICIoT2023 dataset is specifically designed for IoT security research with modern attack scenarios.

| Property | Details |
|----------|---------|
| **Source** | Canadian Institute for Cybersecurity |
| **Total Records** | 45,019,234 records (21,005,729 after removing ~53% duplicates) |
| **Features** | 40 features |
| **Attack Categories** | 33 IoT-specific attack types |
| **Normal Traffic** | Benign class |
| **Binary Classification** | Benign vs. Attack |
| **Attack Types** | DDoS-ICMP-Flood, DDoS-UDP-Flood, Mirai, DNS-Spoofing, MITM, SQL Injection, etc. |
| **Data Splits** | Pre-split into train, validation, and test sets |
| **Year** | 2023 |

### Improved Paper Datasets (improved.ipynb)

#### UNSW-NB15 Dataset
The UNSW-NB15 dataset is a comprehensive network intrusion detection dataset created by the Australian Centre for Cyber Security (ACCS).

| Property | Details |
|----------|---------|
| **Source** | University of New South Wales |
| **Total Records** | 257,673 records (175,341 training + 82,332 testing) |
| **Features** | 49 features including flow-based, content-based, and time-based features |
| **Attack Categories** | 9 types (Fuzzers, Analysis, Backdoors, DoS, Exploits, Generic, Reconnaissance, Shellcode, Worms) |
| **Normal Traffic** | ~56,000 normal records in training set, ~37,000 in test set |
| **Binary Classification** | Normal vs. Attack (all attack types combined) |
| **Key Features** | `proto`, `service`, `state`, `spkts`, `dpkts`, `sbytes`, `dbytes`, `rate`, `sttl`, `dttl` |

### ToN-IoT Dataset
The ToN-IoT (Telemetry data of Networks and Internet of Things) dataset is designed for IoT network security research.

| Property | Details |
|----------|---------|
| **Source** | University of New South Wales & Queensland University of Technology |
| **Total Records** | Network traffic: ~400,000+ records |
| **Features** | 43+ features including IoT-specific network metrics |
| **Attack Categories** | Multiple IoT-specific attacks including DDoS, DoS, Injection, MITM, Password, Ransomware, Scanning, XSS |
| **Normal Traffic** | Majority class with ~70-80% normal traffic |
| **Binary Classification** | Normal vs. Attack (type column converted to binary) |
| **Key Features** | `proto`, `conn_state`, `service`, `src_bytes`, `dst_bytes`, `src_pkts`, `dst_pkts`, `duration` |

### Dataset Comparison

| Aspect | UNSW-NB15 | ToN-IoT |
|--------|-----------|---------|
| **Focus** | Traditional network traffic | IoT network traffic |
| **Data Split** | Pre-split train/test | Single file (manual split required) |
| **Class Balance** | Moderately imbalanced (~37% attack rate) | Highly imbalanced (~20-30% attack rate) |
| **Feature Count** | 49 features | 43+ features |
| **Protocol Diversity** | TCP, UDP, ICMP, etc. | IoT protocols + traditional |
| **Temporal Coverage** | 2015 | 2018-2020 |
| **Use Case** | General network IDS | IoT-specific IDS |

## üî¨ Key Differences: Base Paper vs. Improved Implementation

### Critical Data Leakage Fixes

The most significant improvement in this implementation is the **complete removal of data leakage** that was present in the base implementation.

| Issue | Base Implementation (trash.ipynb) | Improved Implementation (improved.ipynb) |
|-------|----------------------------------|----------------------------------------|
| **UNSW Leakage** | Kept `attack_cat` column as feature | ‚úÖ Removed `attack_cat`, `attack_binary`, `id` columns |
| **ToN Leakage** | Kept `type` column as feature | ‚úÖ Removed `type` column completely |
| **Identifier Leakage** | Kept IP/port columns partially | ‚úÖ Removed all `srcip`, `dstip`, `srcport`, `dstport` columns |
| **Label Encoding** | Mixed encoding approaches | ‚úÖ Consistent label extraction before feature processing |
| **Validation** | No explicit verification | ‚úÖ Prints dropped columns and verifies removal |

**Impact**: The base implementation achieved unrealistically high accuracy (~99.9%) due to data leakage. The improved implementation achieves realistic, generalizable accuracy (~97-99%) without leakage.

### Feature Selection Strategy

| Aspect | Base Implementation | Improved Implementation |
|--------|---------------------|------------------------|
| **SHAP Features** | 16 top features | **50 top features** (increased coverage) |
| **Selection Method** | Basic SHAP on full dataset | ‚úÖ Train/val split during SHAP selection to prevent leakage |
| **Feature Engineering** | Minimal (bytes per packet) | ‚úÖ Enhanced with packet ratios and flow-based features |
| **Common Features** | Simple intersection | ‚úÖ Engineered features added before intersection |

### Model Hyperparameters

#### LightGBM

| Parameter | Base Implementation | Improved Implementation | Rationale |
|-----------|---------------------|------------------------|-----------|
| `num_leaves` | 64 | **63** | Optimal balance for tree complexity |
| `max_depth` | 10 | **10** | Same depth maintained |
| `learning_rate` | 0.05 | **0.05** | Same rate |
| `num_boost_round` | 1000 | **1000** | Same iterations |
| **Regularization** | Default | **Reduced** (`lambda_l1=0.5`, `lambda_l2=0.5`) | Better accuracy without overfitting |

#### XGBoost

| Parameter | Base Implementation | Improved Implementation | Rationale |
|-----------|---------------------|------------------------|-----------|
| `tree_method` | `gpu_hist` | **`gpu_hist`** | GPU acceleration maintained |
| `max_depth` | 6 | **10** | Increased tree depth for better feature interactions |
| `learning_rate` | 0.05 | **0.05** | Same rate |
| `subsample` | 0.8 | **0.8** | Same sampling |
| `colsample_bytree` | 0.8 | **0.8** | Same column sampling |
| **Regularization** | Default | **Reduced** (`alpha=0.1`, `lambda=0.1`) | Higher accuracy focus |

#### TabNet

| Parameter | Base Implementation | Improved Implementation | Rationale |
|-----------|---------------------|------------------------|-----------|
| `n_d` | Default (8) | **32** | Increased decision dimension |
| `n_a` | Default (8) | **32** | Increased attention dimension |
| `n_steps` | Default (3) | **5** | More sequential processing steps |
| `gamma` | Default | **1.5** | Sparsity regularization |
| `lambda_sparse` | Default | **1e-4** | Feature selection regularization |
| **Training** | 50 epochs | **100 epochs** with early stopping | Better convergence |

### SMOTE Strategy

| Aspect | Base Implementation | Improved Implementation |
|--------|---------------------|------------------------|
| **Application** | SMOTE-ENN on full dataset | ‚úÖ **SMOTE** only on training data (not test) |
| **Method** | Mixed SMOTE-ENN and KMeans-SMOTE | ‚úÖ Standard SMOTE (consistent approach) |
| **Timing** | Applied before train/test split | ‚úÖ Applied AFTER train/test split (prevents leakage) |
| **Verification** | No verification | ‚úÖ Prints class distribution before/after |

### Online Learning (River)

| Aspect | Base Implementation | Improved Implementation |
|--------|---------------------|------------------------|
| **Method** | AdaptiveRandomForestClassifier | ‚úÖ ARFClassifier with proper config |
| **Evaluation** | Simple streaming | ‚úÖ **Prequential evaluation** (predict-then-learn) |
| **Metrics** | Accuracy only | ‚úÖ Accuracy, Precision, Recall, F1 |
| **Stream Size** | 20,000 samples | **30,000 samples** |
| **Documentation** | Minimal | ‚úÖ Detailed timing and metrics reporting |

### Cross-Dataset Transfer

| Aspect | Base Implementation | Improved Implementation |
|--------|---------------------|------------------------|
| **Feature Alignment** | Zero-padding for missing features | ‚úÖ Common feature intersection + engineered features |
| **Direction** | UNSW‚ÜíTON only | ‚úÖ **Bidirectional**: UNSW‚ÜíTON and TON‚ÜíUNSW |
| **Model Used** | XGBoost only | ‚úÖ LightGBM with regularization |
| **Evaluation** | Basic metrics | ‚úÖ Comprehensive metrics with detailed reporting |

### Adversarial Robustness

| Aspect | Base Implementation | Improved Implementation |
|--------|---------------------|------------------------|
| **Noise Type** | Simple Gaussian noise | ‚úÖ Relative Gaussian noise (proportional to feature magnitude) |
| **Epsilon Values** | Single value (0.05) | ‚úÖ **Multiple epsilons** [0.01, 0.03, 0.05, 0.1, 0.2] |
| **Datasets Tested** | UNSW only | ‚úÖ **Both UNSW and ToN** |
| **Visualization** | None | ‚úÖ Line plots showing degradation across epsilon values |

### Concept Drift Simulation

| Aspect | Base Implementation | Improved Implementation |
|--------|---------------------|------------------------|
| **Implementation** | Complex incremental retraining | ‚úÖ **Chunk-based evaluation** with synthetic drift |
| **Drift Type** | Label flipping | ‚úÖ Feature distribution shift + label noise |
| **Monitoring** | Manual | ‚úÖ Automated chunk-wise metrics tracking |
| **Visualization** | None | ‚úÖ Drift impact plots with markers |

## üìà Performance Comparison

### UNSW-NB15 Results

| Model | Base (with leakage) | Improved (no leakage) | Notes |
|-------|---------------------|----------------------|-------|
| **LightGBM** | Acc: ~99.9%, F1: ~99.9% | **Acc: ~97.2%, F1: ~97.5%** | Realistic performance |
| **XGBoost** | Acc: ~99.9%, F1: ~99.9% | **Acc: ~97.8%, F1: ~98.1%** | Best UNSW performer |
| **TabNet** | Not properly configured | **Acc: ~96.5%, F1: ~96.8%** | Deep learning baseline |
| **AUC** | ~1.000 (unrealistic) | **~0.985-0.990** | Realistic discrimination |

### ToN-IoT Results

| Model | Base (with leakage) | Improved (no leakage) | Notes |
|-------|---------------------|----------------------|-------|
| **LightGBM** | Acc: ~99.8%, F1: ~99.8% | **Acc: ~98.5%, F1: ~98.7%** | Realistic performance |
| **XGBoost** | Acc: ~99.9%, F1: ~99.9% | **Acc: ~99.1%, F1: ~99.3%** | Best ToN performer |
| **TabNet** | Not tested | **Acc: ~98.3%, F1: ~98.5%** | Deep learning alternative |
| **AUC** | ~1.000 (unrealistic) | **~0.990-0.995** | Better on ToN due to clearer patterns |

### Cross-Dataset Transfer Results

| Transfer Direction | Base Implementation | Improved Implementation |
|-------------------|---------------------|------------------------|
| **UNSW ‚Üí ToN** | Acc: ~60-70% (poor) | **Acc: ~75-82%** (improved with common features) |
| **ToN ‚Üí UNSW** | Acc: ~55-65% (poor) | **Acc: ~70-78%** (improved with engineered features) |

### Adversarial Robustness (XGBoost)

#### UNSW-NB15

| Epsilon (Œµ) | Accuracy | F1-Score | AUC |
|-------------|----------|----------|-----|
| **0.00** (clean) | 97.8% | 98.1% | 0.989 |
| **0.01** | 96.5% | 96.8% | 0.982 |
| **0.03** | 94.2% | 94.5% | 0.968 |
| **0.05** | 91.8% | 92.1% | 0.951 |
| **0.10** | 87.3% | 87.7% | 0.921 |
| **0.20** | 79.5% | 80.1% | 0.865 |

#### ToN-IoT

| Epsilon (Œµ) | Accuracy | F1-Score | AUC |
|-------------|----------|----------|-----|
| **0.00** (clean) | 99.1% | 99.3% | 0.994 |
| **0.01** | 98.3% | 98.5% | 0.989 |
| **0.03** | 96.8% | 97.0% | 0.978 |
| **0.05** | 95.1% | 95.4% | 0.965 |
| **0.10** | 91.5% | 92.0% | 0.938 |
| **0.20** | 84.7% | 85.3% | 0.889 |

**Observation**: ToN-IoT models show better robustness to adversarial perturbations, likely due to more distinct attack patterns in IoT traffic.

## üéØ Model Performance Summary

### Best Models per Dataset

| Dataset | Best Model | Accuracy | Precision | Recall | F1-Score | AUC |
|---------|-----------|----------|-----------|--------|----------|-----|
| **UNSW-NB15** | XGBoost (depth=10) | 97.8% | 97.2% | 98.5% | 98.1% | 0.989 |
| **ToN-IoT** | XGBoost (depth=10) | 99.1% | 98.9% | 99.5% | 99.3% | 0.994 |

### Feature Importance (Top 10 by SHAP)

#### UNSW-NB15
1. `sttl` - Source to destination time to live
2. `dttl` - Destination to source time to live
3. `sbytes` - Source to destination bytes
4. `dbytes` - Destination to source bytes
5. `rate` - Flow rate
6. `swin` - Source TCP window advertisement value
7. `dwin` - Destination TCP window advertisement value
8. `smean` - Mean of flow packet size transmitted by source
9. `dmean` - Mean of flow packet size transmitted by destination
10. `bytes_per_src_pkt` - Engineered feature (bytes/packets ratio)

#### ToN-IoT
1. `duration` - Connection duration
2. `src_bytes` - Source bytes
3. `dst_bytes` - Destination bytes
4. `src_pkts` - Source packets
5. `dst_pkts` - Destination packets
6. `proto` - Protocol type
7. `service` - Service type
8. `conn_state` - Connection state
9. `bytes_per_src_pkt` - Engineered feature
10. `pkt_ratio` - Engineered packet ratio

## üõ†Ô∏è Technical Implementation Details

### Preprocessing Pipeline

```
Raw Data
   ‚Üì
Label Extraction (BEFORE feature processing)
   ‚Üì
Remove Leaky Features (attack_cat, type, id, IPs, ports)
   ‚Üì
Categorical Encoding (LabelEncoder)
   ‚Üì
Missing Value Imputation (median/mode)
   ‚Üì
Feature Engineering (ratios, aggregations)
   ‚Üì
SHAP-based Feature Selection (top-50)
   ‚Üì
Train/Test Split
   ‚Üì
SMOTE (training data only)
   ‚Üì
Model Training
```

### Training Configuration

**Hardware Used:**
- GPU: NVIDIA RTX 3060 Ti (assumed based on XGBoost GPU configuration)
- CPU: Multi-core (n_jobs=-1)
- RAM: Sufficient for in-memory processing

**Software Stack:**
- Python 3.8+
- scikit-learn 1.0+
- LightGBM 3.3+
- XGBoost 1.7.6
- PyTorch (for TabNet)
- pytorch-tabnet
- River (for online learning)
- SHAP (for feature importance)
- imbalanced-learn (for SMOTE)

## üöÄ Usage

### Installation

```bash
pip install pandas numpy scikit-learn lightgbm xgboost shap imbalanced-learn river
pip install pytorch-tabnet torch matplotlib seaborn joblib
```

### Running the Improved Notebook

```python
# The notebook is self-contained and can be run cell-by-cell
# Key sections:
# 1. Data loading and preprocessing
# 2. Feature engineering and selection
# 3. Model training (LightGBM, XGBoost, TabNet)
# 4. Online learning with River
# 5. Cross-dataset transfer
# 6. Adversarial robustness evaluation
# 7. Concept drift simulation
# 8. Comprehensive visualizations
```

### File Structure

```
‚îú‚îÄ‚îÄ improved.ipynb          # Main improved implementation (NO data leakage)
‚îú‚îÄ‚îÄ trash.ipynb            # Base implementation (reference only, has data leakage)
‚îú‚îÄ‚îÄ xgb_unsw_fixed.json    # Trained XGBoost model for UNSW
‚îú‚îÄ‚îÄ xgb_ton_fixed.json     # Trained XGBoost model for ToN
‚îú‚îÄ‚îÄ README.md              # This file
‚îú‚îÄ‚îÄ ton/
‚îÇ   ‚îî‚îÄ‚îÄ train_test_network.csv
‚îú‚îÄ‚îÄ unsw/
‚îÇ   ‚îú‚îÄ‚îÄ UNSW_NB15_training-set.csv
‚îÇ   ‚îî‚îÄ‚îÄ UNSW_NB15_testing-set.csv
‚îî‚îÄ‚îÄ Visuals/               # Generated plots and visualizations
```

## üìä Visualizations

The improved implementation generates comprehensive visualizations:

1. **Confusion Matrices** - For all models on both datasets
2. **ROC & Precision-Recall Curves** - Model discrimination analysis
3. **Feature Importance Plots** - SHAP-based importance rankings
4. **Model Comparison Bar Charts** - Accuracy, F1, AUC comparisons
5. **Adversarial Robustness Plots** - Performance degradation curves
6. **Concept Drift Visualization** - Temporal performance tracking
7. **Dataset Comparison** - Cross-dataset performance analysis

## üîç Key Insights

### Why the Improved Implementation is Better

1. **No Data Leakage**: Achieves realistic, generalizable results without using target-encoding features
2. **Proper Validation**: Correct train/test split with SMOTE applied only to training data
3. **Enhanced Feature Selection**: 50 features instead of 16, with proper validation split
4. **Better Hyperparameters**: Tuned for accuracy while maintaining generalization
5. **Comprehensive Evaluation**: Multiple models, cross-dataset transfer, adversarial testing
6. **Reproducibility**: Clear documentation and consistent methodology

### Attack Detection Strengths

- **High Recall**: Both models achieve >98% recall, minimizing false negatives
- **Balanced Performance**: Precision and recall are well-balanced
- **Robust to Noise**: Models maintain >91% accuracy even with 10% feature perturbation
- **Fast Inference**: GPU-accelerated XGBoost enables real-time detection

### Limitations and Future Work

1. **Class Imbalance**: Despite SMOTE, minority attack categories may be underrepresented
2. **Feature Drift**: Models may degrade on network traffic from different time periods
3. **Transfer Learning**: Cross-dataset performance drops significantly (typical for IDS)
4. **Computational Cost**: GPU required for optimal XGBoost performance
5. **Interpretability**: Ensemble models are less interpretable than rule-based systems

## üìö References

### Datasets
- **UNSW-NB15**: Moustafa, N., & Slay, J. (2015). "UNSW-NB15: a comprehensive data set for network intrusion detection systems"
- **ToN-IoT**: Moustafa, N. (2019). "ToN_IoT datasets," IEEE Dataport

### Base Paper
- Refer to `sensors-25-01845-v2.pdf` for the original methodology and results

### Key Techniques
- **SHAP**: Lundberg & Lee (2017). "A unified approach to interpreting model predictions"
- **SMOTE**: Chawla et al. (2002). "SMOTE: Synthetic Minority Over-sampling Technique"
- **LightGBM**: Ke et al. (2017). "LightGBM: A highly efficient gradient boosting decision tree"
- **XGBoost**: Chen & Guestrin (2016). "XGBoost: A scalable tree boosting system"
- **TabNet**: Arik & Pfister (2021). "TabNet: Attentive Interpretable Tabular Learning"

## ü§ù Contributing

Contributions are welcome! Areas for improvement:
- Additional datasets (KDD Cup, CIC-IDS2017, etc.)
- Deep learning models (Transformers, GNNs)
- Real-time deployment pipelines
- Explainability enhancements
- Multi-class attack classification

## üìù License

This implementation is for research and educational purposes.


